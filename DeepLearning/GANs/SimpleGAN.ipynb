{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bd66bf-47cf-4868-9bd9-6b70ca4b3bbd",
   "metadata": {},
   "source": [
    "# What are GANs ?\n",
    "* **GANs (Generative adversarial Networks)** is a class of machine learning techniques and models used for generating new data samples resembling a given dataset.\n",
    "* They consists of two neural networks- a **Generator** and a **Discriminator**, that competes against eachother in an **adversarial** game-like framework.\n",
    "\n",
    "# How GANs work ?\n",
    "* **Generator(G):** Tries to create realistic fake data.\n",
    "* **Discriminator(D):** Evaluates whether the input data is real(from the actual dataset) or fake (generated by G). This outputs a single scalar.\n",
    "* **Adversarial Training:** Generator improves by trying to fool Discriminator, while Discriminator gets better at detecting fakes. Over time, the Generator produces imcreasingly realistic outputs.\n",
    "\n",
    "#### Note\n",
    "Both the **Discriminator** and the **Generator** start from scratch, meaning they are both *randomly initialized* at the start and then trained simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3786c5-affc-412a-8a7e-f7c155776620",
   "metadata": {},
   "source": [
    "# Training of a Simple GAN\n",
    "In order to implement our very first simple GAN, we need to understand the loss functions of **Generator** and **Discriminator** and how they are used for training.\n",
    "\n",
    "## Training the Discriminator\n",
    "The **Discriminator** needs to be trained in a way that makes it capable to distinguish between the real and fake(generated) images.\n",
    "\n",
    "### Discriminator Loss\n",
    "The Discriminator's objective is to **maximize** the following loss function:\n",
    "$$ L_{D} = \\frac{1}{m} \\sum \\limits _{i=1}^{m} [ \\log{D(x^i)} + \\log(1-D(G(z^i))) ]$$\n",
    "where,\\\n",
    "* $x^i$: real image from the dataset\\\n",
    "* $z^i$: random noise input to the Generator\\\n",
    "* $G(z^i)$: fake image generated by the Generator\n",
    "#### Understanding the two terms\n",
    "* $ \\log D(x^i)$\n",
    "    * Encourages the Discriminator to classify **real images correctly** ($D(x)$ &rarr; $1$).\n",
    "* $\\log(1-D(G(z^i)))$\n",
    "    * Encourages the Discriminator to classify **fake images correctly** ($D(G(z))$ &rarr; $0$).\n",
    "\n",
    "Instead of maximizing $L_D$, we **minimize** its **negative** since optimizers(like Adam or SGD) perform **minimization** by default.\n",
    "\n",
    "## Training the Generator\n",
    "The **Generator(G)** tries to produce fake images that are as realistic as possible, **fooling** the Discriminator into classifying them as real.\n",
    "\n",
    "### Standard Generator Loss (Saturating Version)\n",
    "Initially, we try to minimize:\n",
    "$$L_G = \\frac{1}{m} \\sum \\limits _{i=1}^{m} \\log(1-D(G(z^i)))$$\n",
    "\n",
    "**Issue with this loss function:**\n",
    "* If the **Discriminator becomes too good**, $ùê∑(ùê∫(ùëß))$ will be close to $0$.\n",
    "* Since $\\log(1‚àíD(G(z)))$ flattens near $0$, gradients become **very small** (vanishing gradient problem).\n",
    "* This slows down learning for the Generator.\n",
    "\n",
    "### Standard Generator Loss (Non-Saturating Version)\n",
    "Instead of minimizing $\\log(1-D(G(z^i)))$, we maximize:\n",
    "$$\\log D(G(z))$$\n",
    "This is equivalent to minimizing:\n",
    "$$L_G = - \\log D(G(z))$$\n",
    "* This approach provides **stronger gradients**, allowing the Generator to learn **faster**.\n",
    "\n",
    "#### Note\n",
    "Since both losses involve **binary classification** (real vs. fake), we use **Binary Cross-Entropy Loss (BCELoss)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42bf9f0-7c09-406a-911e-d8a245aed57f",
   "metadata": {},
   "source": [
    "# Implementing a Simple GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d4e99e-55cb-4965-98fb-8e4a5269bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb2da8c-827d-4039-b838-93b25d41e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a638e-6fdd-4b75-a387-46211a6a433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = (1,28,28)\n",
    "batch_size = 32\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f75fd2-adf9-4890-81ad-46674a3f0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,),(0.5,)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395dbe60-a9ad-4d61-a4f2-71650e831ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root=\"dataset/\", transform=data_transforms, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c81907-2777-407a-ab85-5fbd7fa56aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "  def __init__(self,in_features):\n",
    "    super().__init__()\n",
    "    channels, height, width = in_features\n",
    "    self.disc = nn.Sequential(\n",
    "        nn.Conv2d(channels,128,kernel_size=3,stride=1,padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.01),\n",
    "        nn.Conv2d(128,1,kernel_size=3, stride=1,padding=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.disc(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, random_noise_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.ConvTranspose2d(random_noise_dim, 128, kernel_size=7, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1, 1, 1)  # Reshape noise input to match ConvTranspose2d input\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c4477-40dc-41f7-83b1-d0190e1cc409",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210daf3-3826-4408-8835-0923ce25f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_disc = optim.Adam(disc.parameters(),lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(),lr=lr)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36066a-68a5-4530-a43f-f9398fa87be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "  for batch_idx, (real,_) in enumerate(loader):\n",
    "    real = real.to(device)\n",
    "    batch_size = real.shape[0]\n",
    "\n",
    "    random_noise = torch.randn(batch_size,z_dim,1,1).to(device)\n",
    "    fake = gen(random_noise)\n",
    "    disc_real = disc(real).view(-1)\n",
    "    lossD_real = loss_fn(disc_real,torch.ones_like(disc_real))\n",
    "\n",
    "    disc_fake = disc(fake).view(-1)\n",
    "    lossD_fake = loss_fn(disc_fake,torch.zeros_like(disc_fake))\n",
    "    \n",
    "    lossD = (lossD_real + lossD_fake)/2\n",
    "    \n",
    "    disc.zero_grad()\n",
    "    lossD.backward(retain_graph=True)\n",
    "    opt_disc.step()\n",
    "\n",
    "    output = disc(fake).view(-1)\n",
    "    lossG = loss_fn(output,torch.ones_like(output))\n",
    "    gen.zero_grad()\n",
    "    lossG.backward()\n",
    "    opt_gen.step()\n",
    "\n",
    "    if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf03bd-45b0-4e68-a35f-4978dae6e0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
